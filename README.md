# GPT-2 aligment using BERT classifier

## Введение

Данный репозиторий содержит код и результаты серии экспериментов по GPT-2 aligment

Под aligment имеется ввиду дообучение генеративной модели, с целью корректировки результатов генерации в соответствии с нашими ожиданиями. 
Например, мы будем использовать [GPT-2 обученную на imbd датасете](https://huggingface.co/lvwerra/gpt2-imdb), которая генерирует отзывы к фильмам, причем она была обучена как на негативных, так и на положительных отзывах. Мы хотим, чтобы модель генерировала положительные отзывы.

Мы использовали подход [SLiC-HF](https://arxiv.org/pdf/2305.10425.pdf), который является продолжением идеи [Direct Preference Optimization](https://arxiv.org/abs/2305.18290), которая использует методы обучения с подкреплением для того, чтобы добиться от модели "правильной" генерации. SLiC-HF оптимизирует *loss*, который состоит из двух частей, первая "перераспределяет" вероятность генерации ответа на запрос между положительными и отрицательными примерами(или winner-loser парами), а вторая регуляризирует процесс обучения, не допуская сильное отклонение от референсного состояния. Мы будем оценивать тональность отзывов при помощи другой модели и вычислять *reward* для каждого примера.

Мы проведем серию экспериментов для того, чтобы оценить насколько распределение *reward* у генерации модели, которую мы дообучили таким образом, изменится в сравнеии с референсной моделью. Вместе с этим, мы будем следить за разнообразим(*diversity*) текста, которая вычисляется как энтропия токенов генерации модели.

### Requirements

Для запуска скриптов,необходимо поверх стандартного пакета **pytorch**, выполнить следующие команды, для установки и обновления библиотек:
```
pip install -U accelerate
pip install -U transformers
pip install trl
pip install datasets
```
В файле requirements.txt записано окружение в котором выполнялись все скрипты, отмечу, что это стандартное окружение Kaggle ноутбуков с установленными выше библиотеками.


При помощи следующих команд будут сгенерированы данные и подготовлен Dataset:
```
python3 text_generation.py
python3 prepare_dataset.py
```
Для обучения и оценки модели с заданным *loss=hinge/sigmoid* и *beta* нужно запустить скрипт:
```
python3 fine_tune.py loss beta
python3 calculate_metrics.py loss beta
```
Модели будут сохранены в models, будет построено распределение по метрикам и сохраненно в папку results.

## Pipeline

Как было ранее отмечено, в качестве генеративной SFT(supervised fine tuned) модели была использована [GPT-2 обученную на imbd датасете](https://huggingface.co/lvwerra/gpt2-imdb) для генерации отзывов на фильмы. В качестве reward модели мы взяли [BERT](https://huggingface.co/lvwerra/distilbert-imdb) модель, которая классифицирует отзывы на положительные/негативные.

### Генерация

При помощи GPT-2 было сгенерированно 10000 отзывов и посчитан *reward* для каждого, параметры генерации были выбраны:

```
max_length=64,
top_k=50,
top_p=0.85,
```

### Создание датасета

Далее было составлено 20000 пар winner-loser, которые в дальнейшем будут использоваться как датасет для обучения DPO.
Пары выбирались случайным образом, затем тот отзыв у которого выше *reward* выше помечали как winner.

### Дообучение

При помощи [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) (он из коробки может обучаться с SLiC-HF loss), дообучили GPT-2 , также использовали *loss* из оригинальной статьи про DPO

## Результаты

Было проведенно десять экспериментов для каждой из моделей, с генерацией 5000 примеров для вычисления энтропии текста и средней награды.

Для подсчета результатов был использован скрипт:
```
python3 process_results.py
```
Ниже таблица с результатами

<table>
<thead>
  <tr>
    <th>loss<br></th>
    <th>beta</th>
    <th>reward (std)</th>
    <th>diversity (std)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td colspan="2">Original Model</td>
    <td>0.507(0.21)</td>
    <td>5.975(0.007)</td>
  </tr>
  <tr>
    <td rowspan="2">hinge</td>
    <td>0.1</td>
    <td>2.738(0.005)</td>
    <td>4.193(0.003)<br></td>
  </tr>
  <tr>
    <td>1</td>
    <td>2.240(0.014)<br></td>
    <td>4.522(0.005)<br></td>
  </tr>
  <tr>
    <td rowspan="2">sigmoid</td>
    <td>0.1</td>
    <td>2.712(0.0017)</td>
    <td>4.150(0.0025)</td>
  </tr>
  <tr>
    <td>1</td>
    <td>2.718(0.0054)</td>
    <td>4.109(0.006)<br></td>
  </tr>
</tbody>
</table>
В целом, во всех случаях добились генерации положительных отзывов.

Видно, что при *beta = 0.1* результат с *loss = hinge* оказался немного лучше, чем с *loss = sigmoid* по обеим метрикам. 
Однако, при значении *beta = 1.0* модель с "loss = hinge" показывает значительно лучшее разнообразие, за которое пришлось заплатить средним значением *reward*, что ожидаемо, так как *beta* параметр регуляризации, который не позволяет модели сильно отклониться от исходной, тем самым сохраняя разнообразие токенов при более высоком значении параметра регуляризации.

Ниже для наглядности также приведены несколько графиков с распределением *reward*:

![Image alt](https://github.com/Revelia/GPT2-aligment/blob/master/images/result.jpg)

## Выбор пар winner-loser

Мы могли бы выбирать не случайные пары winner-looser, а лишь те, между которыми есть какое то "достаточное" различие с точки зрения *reward* функции, в условиях ограниченных вычислительных ресурсов это может привести к улучшению результатов генерации, так как на данный момент, мы достаточно часто можем перекидывать вероятность с "менее положительного" отзыва в "более положительный". Вероятно, такой подход увеличит *diversity* модели, но возможно уменьшит срединй reward. Проверим это.

Мы повторили эксперименты, добавляя в датасет лишь такие пары, что *reward(winner) - reward(loser) > 1.0*. Результаты выглядят следующим образом:

<table class="iksweb">
	<tbody>
		<tr>
			<td>loss</td>
			<td>beta</td>
			<td>reward</td>
			<td>diversity</td>
		</tr>
		<tr>
			<td colspan="2">Original Model</td>
			<td>0.507</td>
			<td>5.975</td>
		</tr>
		<tr>
			<td rowspan="2">hinge</td>
			<td>0.1</td>
			<td>2.603</td>
			<td>4.268</td>
		</tr>
		<tr>
			<td>1.0</td>
			<td>2.405</td>
			<td>4.331</td>
		</tr>
		<tr>
			<td rowspan="2">sigmoid</td>
			<td>0.1</td>
			<td>2.481</td>
			<td>4.203</td>
		</tr>
		<tr>
			<td>1.0</td>
			<td>2.477</td>
			<td>4.195</td>
		</tr>
	</tbody>
</table>

Еще лучше разницу можно наблюдать на распределениях. 

![Image alt](https://github.com/Revelia/GPT2-aligment/blob/master/images/result%20margin.jpg)

Заметим, что плотность положительных отзывов "расползается" по большему интервалу значений. Естественным будет предположение, что если интервал на котором находится большая часть отзывов шире, то разнообразие генерируемых токенов становится выше, что подтверждается результатами экспериментов.

Заметим также, что, например, за высокий diversity с "hinge = 1.0" мы заплатили достаточно заметной генерацией сильно негативных отзывов, а с новой стратегией выбора самые негативные отзывы находятся в нейтральной зоне. Кажется, помимо метрики средней награды, нам стоит также следить за чем то вроде accuracy тональности генерируемых отзывов, где мы считаем отзыв негативным если он имеет *reward* ниже определенной границы.

Попробуем еще одну стратегию. В предыдущем варианте мы все еще могли выбирать пары положительных отзывов. Будем теперь требовать, чтобы один из отзывов обязательно имел *reward* < 0.5 (взять как среднее значение reward у модели), а второй соответственно выше. Хочется как можно более шире "размазать" плотность положительных отзывов не вызывая при этом генерации крайне негативных.

Результаты: 

<table class="iksweb">
	<tbody>
		<tr>
			<td>loss</td>
			<td>beta</td>
			<td>reward</td>
			<td>diversity</td>
		</tr>
		<tr>
			<td colspan="2">Original Model</td>
			<td>0.507</td>
			<td>5.975</td>
		</tr>
		<tr>
			<td rowspan="2">hinge</td>
			<td>0.1</td>
			<td>2.247</td>
			<td>4.185</td>
		</tr>
		<tr>
			<td>1.0</td>
			<td>2.308</td>
			<td>4.205</td>
		</tr>
		<tr>
			<td rowspan="2">sigmoid</td>
			<td>0.1</td>
			<td>2.519</td>
			<td>4.294</td>
		</tr>
		<tr>
			<td>1.0</td>
			<td>2.462</td>
			<td>4.334</td>
		</tr>
	</tbody>
</table>

Заметное улучшение только для случая *signoid 1.0*, в остальном стало даже хуже. Мы скорее получили эффект 'смещения' центра тяжести плотности, а не равномерного размазывания.

## Что можно улучшить?
Есть предположение, что так как у оригинальной модели есть три ярко выраженных пика у плотности логитов, то когда мы семплируем сталкиваемся с дисбалансом наших примеров. Можно ожидать, что если более равномерно размазать плотность по интервалу, например, [0, 2.5], то разнообразие текстов все таки увеличится. 

Также можно попробовать таким методом попытаться решить другую задачу: как было выше отмечено, у плотности распределения логитов оригинальной модели есть три ярко выраженных пика. Что, если попытаться размазать вероятность более равномерно по интервалу? Увеличит ли это энтропию токенов в сравнении с оригинальной моделью? Интересно было бы это тоже проверить.


