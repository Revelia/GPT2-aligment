# GPT-2 aligment using BERT classifier

## Введение

Данный репозиторий содержит код и результаты серии экспериментов по GPT-2 aligment

Под aligment имеется ввиду дообучение генеративной модели с целью корректировки результатов генерации в соответствии с нашими ожиданиями. 
Например, мы будем использовать [GPT-2 обученную на imbd датасете](https://huggingface.co/lvwerra/gpt2-imdb), которая генерирует отзывы к фильмам, причем она была обучена как на негативных так и на положительных отзывах. Мы хотим, чтобы модель генерировала положительные отзывы.

Мы использовали подход [SLiC-HF](https://arxiv.org/pdf/2305.10425.pdf), который является продолжением идеи [Direct Preference Optimization](https://arxiv.org/abs/2305.18290), которая использует методы обучения с подкреплением для того, чтобы добиться от модели "правильной" генерации. SLiC-HF оптимизирует loss, который состоит из двух частей, первая "перераспределяет" вероятность генерации ответа на запрос между положительными и отрицательными примерами(или winner-loser парами), а вторая регуляризирует процесс обучения, не допуская сильное отклонение от референсного состояния. Мы будем оценивать тональность отзывов при помощи другой модели и вычислять reward для каждого примера.

Мы проведем серию экспериментов для того, чтобы оценить насколько распределение reward у генерации модели которую мы дообучили таким образм изменится в сравнеии с референсной моделью. Также интересно проследить на то, как меняется метрика diversity и оценить влияение гиперпараметров SLiC-HF.

### Requirements

Для запуска скриптов необходимо выполнить следующие команды для установки и обновления библиотек:
```
pip install -U accelerate
pip install -U transformers
pip install trl
pip install datasets
```

При помощи следующих команд будут сгенерированы данные и подготовлен Dataset
```
python3 text_generation.py
python3 prepare_dataset.py
```
Для обучения и оценки модели с заданным loss=hinge/sigmoid нужно запустить скрипт
```
python3 main.py loss
python3 calculate_metrics.py loss
```
Модели будут сохранены в models, будет построено распределение по метрикм и сохранено в папку results.

## Pipeline

Как было ранее отмечено, в качестве генеративной SFT(supervised fine tuned) модели была использована [GPT-2 обученную на imbd датасете](https://huggingface.co/lvwerra/gpt2-imdb) для генерации отзывов на фильмы. В качестве reward модели мы взяли [BERT](https://huggingface.co/lvwerra/distilbert-imdb) модель, которая классифицирует отзывы на положительные/негативные.

### Генерация

При помощи GPT-2 было сгенерированно 1000 отзывов и посчитан reward для каждого (generate_text.py

### Создание датасета

Далее было составлено 2000 пар winner-loser, которые в дальнейшем будут использоваться как датасет для обучения DPO (prepare_dataset.py) 

### Дообучение

При помощи [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) (он из коробки может обучаться с SLiC-HF loss), дообучили GPT-2 (main.py), также использовали loss из оригинальной статьи про DPO

### Вычисление метрик

Посчитали метрики, распределение отзывов по reward, среднее значение reward а также diversity текстов до и после.

## Результаты

TODO

![Image alt](https://github.com/Revelia/GPT2-aligment/blob/master/images/result.jpg)
