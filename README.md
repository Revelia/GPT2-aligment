# GPT-2 aligment using BERT classifier

## Введение

Данный репозиторий содержит код и результаты серии экспериментов по GPT-2 aligment

Под aligment имеется ввиду дообучение генеративной модели, с целью корректировки результатов генерации в соответствии с нашими ожиданиями. 
Например, мы будем использовать [GPT-2 обученную на imbd датасете](https://huggingface.co/lvwerra/gpt2-imdb), которая генерирует отзывы к фильмам, причем она была обучена как на негативных, так и на положительных отзывах. Мы хотим, чтобы модель генерировала положительные отзывы.

Мы использовали подход [SLiC-HF](https://arxiv.org/pdf/2305.10425.pdf), который является продолжением идеи [Direct Preference Optimization](https://arxiv.org/abs/2305.18290), которая использует методы обучения с подкреплением для того, чтобы добиться от модели "правильной" генерации. SLiC-HF оптимизирует *loss*, который состоит из двух частей, первая "перераспределяет" вероятность генерации ответа на запрос между положительными и отрицательными примерами(или winner-loser парами), а вторая регуляризирует процесс обучения, не допуская сильное отклонение от референсного состояния. Мы будем оценивать тональность отзывов при помощи другой модели и вычислять *reward* для каждого примера.

Мы проведем серию экспериментов для того, чтобы оценить насколько распределение *reward* у генерации модели, которую мы дообучили таким образом, изменится в сравнеии с референсной моделью. Вместе с этим, мы будем следить за разнообразим(*diversity*) текста, которая вычисляется как энтропия токенов генерации модели.

### Requirements

Для запуска скриптов необходимо поверх стандартного пакета **pytorch** выполнить следующие команды для установки и обновления библиотек:
```
pip install -U accelerate
pip install -U transformers
pip install trl
pip install datasets
```
В файле requirements.txt записано окружение в котором выполнялись все скрипты, отмечу, что это стандартное окружение Kaggle ноутбуков с установленными выше библиотеками.


При помощи следующих команд будут сгенерированы данные и подготовлен Dataset:
```
python3 text_generation.py
python3 prepare_dataset.py
```
Для обучения и оценки модели с заданным *loss=hinge/sigmoid* и *beta* нужно запустить скрипт:
```
python3 fine_tune.py loss beta
python3 calculate_metrics.py loss beta
```
Модели будут сохранены в models, будет построено распределение по метрикм и сохранено в папку results.

## Pipeline

Как было ранее отмечено, в качестве генеративной SFT(supervised fine tuned) модели была использована [GPT-2 обученную на imbd датасете](https://huggingface.co/lvwerra/gpt2-imdb) для генерации отзывов на фильмы. В качестве reward модели мы взяли [BERT](https://huggingface.co/lvwerra/distilbert-imdb) модель, которая классифицирует отзывы на положительные/негативные.

### Генерация

При помощи GPT-2 было сгенерированно 10000 отзывов и посчитан *reward* для каждого, параметры генерации были выбраны:

```
max_length=64,
top_k=50,
top_p=0.85,
```

### Создание датасета

Далее было составлено 20000 пар winner-loser, которые в дальнейшем будут использоваться как датасет для обучения DPO.
Пары выбирались случайным образом, затем тот отзыв у которого выше *reward* выше помечали как winner.

### Дообучение

При помощи [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) (он из коробки может обучаться с SLiC-HF loss), дообучили GPT-2 , также использовали *loss* из оригинальной статьи про DPO

## Результаты

Было проведенно десять экспериментов для каждой из моделей, с генерацией 5000 примеров для вычисления энтропии текста и средней награды.

Для подсчета результатов был использован скрипт:
```
python3 process_results.py
```
Ниже таблица с результатами

<table>
<thead>
  <tr>
    <th>loss<br></th>
    <th>beta</th>
    <th>reward (std)</th>
    <th>diversity (std)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td colspan="2">Original Model</td>
    <td>0.507(0.21)</td>
    <td>5.975(0.007)</td>
  </tr>
  <tr>
    <td rowspan="2">hinge</td>
    <td>0.1</td>
    <td>2.738(0.005)</td>
    <td>4.193(0.003)<br></td>
  </tr>
  <tr>
    <td>1</td>
    <td>2.240(0.014)<br></td>
    <td>4.522(0.005)<br></td>
  </tr>
  <tr>
    <td rowspan="2">sigmoid</td>
    <td>0.1</td>
    <td>2.712(0.0017)</td>
    <td>4.150(0.0025)</td>
  </tr>
  <tr>
    <td>1</td>
    <td>2.718(0.0054)</td>
    <td>4.109(0.006)<br></td>
  </tr>
</tbody>
</table>
В целом, во всех случаях добились генерации положительных отзывов.

Видно, что при *beta = 0.1* результат с *loss = hinge* оказался немного лучше чем с *loss = sigmoid* по обеим метрикам. 
Однако, при значении *beta = 1.0* модель с "loss = hinge" показывает значительно лучшее разнообразие, за которое пришлось заплатить средним значением *reward*, что ожидаемо, так как *beta* параметр регуляризации, который не позволяет модели сильно отклониться от исходной, тем самым сохраняя разнообразие токенов при более высоком значении параметра регуляризации.

Ниже для наглядности также приведены несколько графиков с распределением *reward*:

![Image alt](https://github.com/Revelia/GPT2-aligment/blob/master/images/result.jpg)

## Что можно улучшить?
К сожалению из за нехватки времени не удалось успеть провести эксперименты с заменой *loss*, однако в текущем пайплайне есть следующая идея:

Мы могли бы выбирать не случайные пары winner-looser, а лишь те, между которыми есть какое то "достаточное" различие с точки зрения *reward* функции, в условиях ограниченных вычислительных ресурсов это может привести к улучшению результатов генерации, так как на данный момент, мы достаточно часто можем перекидывать вероятность с "менее положительного" отзыва в "более положительный". Вероятно, такой подход увеличит *diversity* модели, но возможно уменьшит срединй reward, но эту гипотезу нужно проверять.
